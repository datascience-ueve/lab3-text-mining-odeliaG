{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patricia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk    #Natural language processing tool-kit\n",
    "nltk.download('stopwords')\n",
    "  \n",
    "from nltk.corpus import stopwords                   #Stopwords corpus\n",
    "\n",
    "from keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test)  = imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification des dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Taille X_train: (25000,) \n",
      " Taille X_test: (25000,) \n",
      " Taille Y_train: (25000,) \n",
      " Taille Y_test: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\" Taille X_train:\",np.shape(X_train),\"\\n\",\n",
    "      \"Taille X_test:\",np.shape(X_test),\"\\n\",\n",
    "      \"Taille Y_train:\",np.shape(Y_train),\"\\n\",\n",
    "      \"Taille Y_test:\",np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(np.concatenate((X_train, X_test)))\n",
    "y = pd.DataFrame(np.concatenate((Y_train, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Labels : [0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\" Labels :\" ,np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit d'un problème de classification binaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Y_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[24999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class-Balancing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On écrit une fonction permettant de vérifier que les classes sont bien équilibrées dans le train et le test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classBalancing(labelVector):\n",
    "    nbr_0  = 0\n",
    "    nbr_1 = 0\n",
    "    seq = np.shape(labelVector)[0]\n",
    "    i = 0\n",
    "    for i in range(seq):\n",
    "        if labelVector[i] == 0:\n",
    "            nbr_0 = nbr_0 + 1\n",
    "        else:\n",
    "            nbr_1 = nbr_1 + 1 \n",
    "    return nbr_0,nbr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de 0 et de 1 dans le train : (12500, 12500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre de 0 et de 1 dans le train :\",classBalancing(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de 0 et de 1 dans le test : (12500, 12500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre de 0 et de 1 dans le test :\",classBalancing(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashage - déHashage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par remarquer que les textes ont déja été hashés : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On écrit une fonction pour reconstruite le texte (avec les mots les plus fréquents uniquement) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeysByValue(dictOfElements, valueToFind):\n",
    "    listOfKeys = list()\n",
    "    listOfItems = dictOfElements.items()\n",
    "    for item  in listOfItems:\n",
    "        if item[1] == valueToFind:\n",
    "            listOfKeys.append(item[0])\n",
    "    print(listOfKeys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifions que ça fonctionne pour le sixième commentaire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the']\n",
      "['effort']\n",
      "['still']\n",
      "['been']\n",
      "['that']\n",
      "['usually']\n",
      "['makes']\n",
      "['for']\n",
      "['of']\n",
      "['finished']\n",
      "['sucking']\n",
      "['ended']\n",
      "[\"cbc's\"]\n",
      "['an']\n",
      "['because']\n",
      "['before']\n",
      "['if']\n",
      "['just']\n",
      "['though']\n",
      "['something']\n",
      "['know']\n",
      "['novel']\n",
      "['female']\n",
      "['i']\n",
      "['i']\n",
      "['slowly']\n",
      "['lot']\n",
      "['of']\n",
      "['above']\n",
      "['freshened']\n",
      "['with']\n",
      "['connect']\n",
      "['in']\n",
      "['of']\n",
      "['script']\n",
      "['their']\n",
      "['that']\n",
      "['out']\n",
      "['end']\n",
      "['his']\n",
      "['deceptively']\n",
      "['i']\n",
      "['i']\n"
     ]
    }
   ],
   "source": [
    "for word_num in X_train[5]:\n",
    "    getKeysByValue(imdb.get_word_index(),word_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plutôt que d'effectuer la transformation entier par entier, transformons tout un vecteur (donc toute une ligne de notre X_train par exemple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorToSentence(Vector):\n",
    "    for word_num in Vector:\n",
    "        getKeysByValue(imdb.get_word_index(),word_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Derniere\" étape : on transforme les données pleines de chiffres illisibles en mots : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intToReviews(data):\n",
    "    k = 0\n",
    "    for k in range (np.shape(data)[0]):\n",
    "        data_txt = vectorToSentence(data[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant on va créer un dictionnaire avec tous les mots de tous les commentaires de nos données d'apprentissage. Puis on va compter le nombre d'apparition de chaque mot du dictionnaire dans chacune des phrases des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont trés volumineuses, ne pouvant pas compiler sur mon ordi, je vous donne quand même le code en esperant qu'il n'y a pas trop d'erreurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_txt = intToReviews(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer() # transforme text en vecteur\n",
    "X_counts = count_vect.fit_transform(X_train_txt)\n",
    "dictionary_sub   = count_vect.get_feature_names()\n",
    "print(np.shape(dictionary_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On écrit une fonction qui prend en arguments la vectorization et les données d'apprentissage.\n",
    "On crée un dictionnaire à partir de ces données.\n",
    "on crée un data frame à partir de la matrice sparse qui compte la présence de chaque mot du dictionnaire dans chaque phrase des données d'apprentissage : word_counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "\n",
    "def most_frequent_words(count_vect,data):  \n",
    "    \n",
    "    data_counts = count_vect.fit_transform(data)\n",
    "    dictionary   = count_vect.get_feature_names()\n",
    "    print(\"Taille du dictionnaire : \",len(dictionary))\n",
    "    \n",
    "    word_counts = pd.DataFrame(data_counts.toarray(),columns = dictionary)\n",
    "    word_total_counts = pd.DataFrame()\n",
    "    word_total_counts['count'] = word_counts.sum(axis=0)\n",
    "    word_total_counts['word'] = dictionary\n",
    "    \n",
    "    word_total_counts_sorted_top  = word_total_counts.sort_values(by=\"count\",ascending = False).iloc[0:20]\n",
    "    \n",
    "    plt.figure(figsize=(15,8))\n",
    "    sns.barplot(x = word_total_counts_sorted_top.index,y=word_total_counts_sorted_top['count'])\n",
    "    plt.xticks(rotation=90,fontsize=10)\n",
    "    plt.show()\n",
    "    \n",
    "    return {'data_counts' : data_counts, 'word_total_counts' :word_total_counts, 'dictionary' : dictionary}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"doesn't\", 'because', 'both', \"hadn't\", 'did', 'out', 'it', 'your', \"that'll\", 'she', 'its', 'these', 'by', \"it's\", \"should've\", 'further', 'his', 'm', 'have', \"couldn't\", \"you're\", 'doesn', 'up', 'can', 'with', 'which', 'own', 'the', \"shan't\", \"you'll\", \"wouldn't\", 'all', 've', 'being', 'won', 'during', 'do', \"won't\", 'whom', 'an', 'once', 'and', 'into', 're', 'too', 'each', 'theirs', 'we', 'over', 'does', 'hasn', 'was', 'here', 'not', 'our', 'how', 'than', \"she's\", 'wouldn', 'very', 'but', 'through', 'd', 'me', 'again', 'haven', 'now', 'needn', 'just', 'wasn', \"wasn't\", 'them', 'they', \"aren't\", 'such', 'in', 'yours', 'didn', 'down', 'been', 'yourself', 'few', 'from', 'there', 'at', 'what', 'no', 'isn', 'mightn', 'why', 'while', 'yourselves', 'off', 'having', 'under', 'about', 'most', 'don', \"needn't\", 'had', 'aren', 'myself', 'itself', 'couldn', 'to', 'any', 'you', \"hasn't\", 'were', 'before', 'below', 'where', 'is', 'he', 'ours', 'other', 'should', 'then', 'hadn', 't', 'are', 'am', \"mightn't\", 'same', 'themselves', 'him', 'their', 'who', 'i', 'more', 'some', 'her', 'doing', 'shan', \"weren't\", 'only', 'nor', 'hers', 'herself', 'this', \"didn't\", 'that', 'a', 'ma', 'when', 'weren', 'against', 'ain', 'or', 'will', 'those', 'after', 'himself', \"you'd\", 'be', 'between', 'if', 'so', 'ourselves', \"you've\", 'll', 'on', 'y', \"isn't\", 'mustn', 'o', 'shouldn', 'my', 'until', 'as', 'of', \"don't\", \"mustn't\", 'for', \"haven't\", 'has', 'above', 's', \"shouldn't\"}\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english')) \n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_sw = CountVectorizer(stop_words = stop)\n",
    "most_frequent_words_sw = most_frequent_words(count_vect_sw,X_train_txt) # enleve les mots non pertinents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop.add('movie')\n",
    "stop.add('film')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation du corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule l'histogramme : proportion de chacun des mots du dictionnaire $ h_w(d) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent = np.array(most_frequent_words_sw['data_counts'].toarray())\n",
    "hist = most_frequent / most_frequent.sum(axis=1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist[np.argsort(df['label']),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(hist[np.argsort(df['label']),:])\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_transf = transformer.fit_transform(most_frequent_words_sw['data_counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(tf_idf_transf.toarray()[np.argsort(df['label']),:])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur le count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_counts = MultinomialNB()\n",
    "NB_counts.fit(X=most_frequent_words_sw['data_counts'],y=df['label'])\n",
    "accuracy_score(NB_counts.predict(most_frequent_words_sw['data_counts']),df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur le TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_tfidf = GaussianNB()\n",
    "NB_tfidf.fit(X=tf_idf_transf.toarray(),y=df['label'])\n",
    "accuracy_score(NB_tfidf.predict(tf_idf_transf.toarray()),df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur le count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    lr_counts = LogisticRegression(C=c)\n",
    "    lr_counts.fit(X=most_frequent_words_sw['data_counts'],y=df['label'])\n",
    "    print (\"Accuracy for C= %s: %s\"  %(c, accuracy_score(lr_counts.predict(most_frequent_words_sw['data_counts']),df['label'])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sur le TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    lr_tfidf = LogisticRegression(C=c)\n",
    "    lr_tfidf.fit(X=most_frequent_words_sw['data_counts'],y=df['label'])\n",
    "    print (\"Accuracy for C= %s: %s\"  %(c, accuracy_score(lr_tfidf.predict(tf_idf_transf.toarray()),df['label'])))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
